import os
from pathlib import Path
from typing import List
import unicodedata
import re

import pandas as pd


# ================== C·∫§U H√åNH C∆† B·∫¢N ==================

BASE_DIR = Path(__file__).resolve().parent

GOOGLE_FILE = BASE_DIR / "google_api_scraper" / \
    "googlebooks_vi_multi_enriched.xlsx"
EBOOKVIE_FILE = BASE_DIR / "ebookvie" / "ebookvie_books.xlsx"
OUTPUT_FILE = BASE_DIR / "merged_books.xlsx"

STANDARD_COLUMNS: List[str] = [
    "title",
    "author_name",
    "categories",
    "description",
    "published_date",
    "language",
    "publisher",
    "page_count",
    "isbn_10",
    "isbn_13",
    "cover_url",
    "info_link",
    "source_url",
    "source",
]


# ================== H√ÄM TI·ªÜN √çCH ==================

def ensure_columns(df: pd.DataFrame, required_cols: List[str]) -> pd.DataFrame:
    """
    ƒê·∫£m b·∫£o DataFrame c√≥ ƒë·ªß c√°c c·ªôt trong required_cols.
    N·∫øu thi·∫øu c·ªôt n√†o th√¨ th√™m c·ªôt ƒë√≥ v·ªõi gi√° tr·ªã None.
    """
    for col in required_cols:
        if col not in df.columns:
            df[col] = None
    return df


def normalize_text_columns(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    """
    Chu·∫©n ho√° c√°c c·ªôt text: fillna(''), √©p string, strip kho·∫£ng tr·∫Øng.
    """
    for col in cols:
        if col in df.columns:
            df[col] = df[col].fillna("").astype(str).str.strip()
    return df


def normalize_for_dedup(text: str) -> str:
    """
    Chu·∫©n ho√° chu·ªói ƒë·ªÉ so s√°nh tr√πng:
    - lower()
    - b·ªè d·∫•u ti·∫øng Vi·ªát
    - ch·ªâ gi·ªØ a-z, 0-9, kho·∫£ng tr·∫Øng
    - gom nhi·ªÅu space v·ªÅ 1 space
    """
    if not isinstance(text, str):
        text = "" if pd.isna(text) else str(text)

    text = text.strip().lower()
    if not text:
        return ""

    # B·ªè d·∫•u: NFD r·ªìi lo·∫°i c√°c k√Ω t·ª± Mn (mark nonspacing)
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")

    # Ch·ªâ gi·ªØ a-z, 0-9, space
    text = re.sub(r"[^a-z0-9]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# ================== H√ÄM CH√çNH ==================

def merge_books(
    google_file: Path = GOOGLE_FILE,
    ebookvie_file: Path = EBOOKVIE_FILE,
    output_file: Path = OUTPUT_FILE,
) -> None:
    # ====== 1. KI·ªÇM TRA FILE ======
    if not google_file.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {google_file}")
        return
    if not ebookvie_file.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {ebookvie_file}")
        return

    # ====== 2. ƒê·ªåC EXCEL ======
    print(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ Google Books: {google_file}")
    df_google = pd.read_excel(google_file)

    print(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ ebookvie: {ebookvie_file}")
    df_ebookvie = pd.read_excel(ebookvie_file)

    # ====== 3. CHU·∫®N HO√Å GOOGLE BOOKS ======
    df_google = ensure_columns(df_google, STANDARD_COLUMNS)

    # N·∫øu Google ch∆∞a c√≥ source_url th√¨ map t·ª´ info_link
    if "source_url" not in df_google.columns or df_google["source_url"].isna().all():
        df_google["source_url"] = df_google["info_link"]

    df_google["source"] = "google_books"

    # ====== 4. CHU·∫®N HO√Å EBOOKVIE ======
    if "cover_url" not in df_ebookvie.columns:
        if "image_url" in df_ebookvie.columns:
            df_ebookvie["cover_url"] = df_ebookvie["image_url"]
        else:
            df_ebookvie["cover_url"] = None

    if "language" not in df_ebookvie.columns:
        df_ebookvie["language"] = "vi"

    df_ebookvie = ensure_columns(df_ebookvie, STANDARD_COLUMNS)
    df_ebookvie["source"] = "ebookvie"

    # ====== 5. G·ªòP 2 DATAFRAME ======
    df_all = pd.concat([df_google, df_ebookvie], ignore_index=True)

    # Chu·∫©n ho√° text cho m·ªôt s·ªë c·ªôt quan tr·ªçng
    df_all = normalize_text_columns(
        df_all, ["title", "author_name", "isbn_10", "isbn_13"]
    )

    # Th√™m c·ªôt normalized cho title & author ƒë·ªÉ d√πng trong dedup kh√¥ng d·∫•u
    df_all["norm_title"] = df_all["title"].apply(normalize_for_dedup)
    df_all["norm_author_name"] = df_all["author_name"].apply(
        normalize_for_dedup)

    print("T·ªïng s·ªë b·∫£n ghi tr∆∞·ªõc khi lo·∫°i tr√πng:", len(df_all))

    # ====== 6. LO·∫†I TR√ôNG ======
    # Chia 2 nh√≥m: c√≥ isbn_13 v√† kh√¥ng
    df_has_isbn = df_all[df_all["isbn_13"] != ""].copy()
    df_no_isbn = df_all[df_all["isbn_13"] == ""].copy()

    # Nh√≥m c√≥ ISBN_13: dedup theo isbn_13
    before_has = len(df_has_isbn)
    df_has_isbn = df_has_isbn.drop_duplicates(subset=["isbn_13"])
    after_has = len(df_has_isbn)
    print(f"Lo·∫°i tr√πng nh√≥m c√≥ ISBN_13: {before_has} -> {after_has}")

    # Nh√≥m kh√¥ng c√≥ ISBN_13:
    # üëâ dedup theo norm_title + norm_author_name (kh√¥ng d·∫•u, lowercase)
    before_no = len(df_no_isbn)
    df_no_isbn = df_no_isbn.drop_duplicates(
        subset=["norm_title", "norm_author_name"]
    )
    after_no = len(df_no_isbn)
    print(
        f"Lo·∫°i tr√πng nh√≥m kh√¥ng c√≥ ISBN_13 (title + author, ƒë√£ b·ªè d·∫•u): "
        f"{before_no} -> {after_no}"
    )

    # G·ªôp l·∫°i
    df_merged = pd.concat([df_has_isbn, df_no_isbn], ignore_index=True)
    print("T·ªïng s·ªë b·∫£n ghi sau khi lo·∫°i tr√πng:", len(df_merged))

    # B·ªè c√°c c·ªôt norm d√πng n·ªôi b·ªô
    df_merged.drop(columns=["norm_title", "norm_author_name"],
                   inplace=True, errors="ignore")

    # ƒê·∫£m b·∫£o ƒë√∫ng th·ª© t·ª± c·ªôt & sort cho d·ªÖ nh√¨n
    df_merged = ensure_columns(df_merged, STANDARD_COLUMNS)
    df_merged = df_merged[STANDARD_COLUMNS]

    df_merged = df_merged.sort_values(
        by=["title", "author_name"], ascending=True)

    # ====== 7. L∆ØU FILE K·∫æT QU·∫¢ ======
    df_merged.to_excel(output_file, index=False)
    print(f"‚úÖ ƒê√£ l∆∞u file g·ªôp: {output_file}")


def main():
    merge_books()


if __name__ == "__main__":
    main()
