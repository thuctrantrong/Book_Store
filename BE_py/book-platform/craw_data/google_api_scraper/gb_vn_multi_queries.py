import os
import time
import html
import requests
import pandas as pd
from typing import Dict, Any, List, Tuple


# ================== C·∫§U H√åNH C∆† B·∫¢N ==================

BASE_URL = "https://www.googleapis.com/books/v1/volumes"
MAX_PER_REQUEST = 40          # Google cho t·ªëi ƒëa 40
DEFAULT_SLEEP_SECONDS = 1.0
DEFAULT_LANG = "vi"


# ================== H√ÄM TI·ªÜN √çCH ==================

def get_api_key() -> str:
    """
    L·∫•y API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng GOOGLE_BOOKS_API_KEY.
    N·∫øu kh√¥ng c√≥ th√¨ fallback sang key hard-code.
    """
    api_key = os.getenv("GOOGLE_BOOKS_API_KEY", "").strip()
    if not api_key:
        # Fallback key b·∫°n ƒëang d√πng ƒë·ªÉ test
        api_key = "AIzaSyCRX14XD2udNBdUlmxfpIgLoKetVxbtqQ4"

    if not api_key:
        raise RuntimeError(
            "Ch∆∞a c·∫•u h√¨nh GOOGLE_BOOKS_API_KEY.\n"
            "H√£y set bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c s·ª≠a l·∫°i get_api_key()."
        )
    return api_key


def extract_isbns(volume: Dict[str, Any]) -> Tuple[str, str]:
    """L·∫•y ISBN_10, ISBN_13 t·ª´ volumeInfo.industryIdentifiers (n·∫øu c√≥)."""
    industry_ids = volume.get("industryIdentifiers") or []
    isbn_10, isbn_13 = "", ""

    for ident in industry_ids:
        id_type = ident.get("type") or ""
        identifier = ident.get("identifier") or ""
        if id_type == "ISBN_10":
            isbn_10 = identifier
        elif id_type == "ISBN_13":
            isbn_13 = identifier

    return isbn_10, isbn_13


def extract_cover_url(volume: Dict[str, Any]) -> str:
    """
    L·∫•y URL cover t·ª´ volumeInfo.imageLinks.
    ∆Øu ti√™n thumbnail -> smallThumbnail.
    """
    image_links = volume.get("imageLinks", {}) or {}

    cover_url = (
        image_links.get("thumbnail")
        or image_links.get("smallThumbnail")
        or ""
    )

    # G·ª° &amp; n·∫øu c√≥
    cover_url = html.unescape(cover_url or "")

    return cover_url


def extract_book_from_item(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    R√∫t g·ªçn 1 item Google Books th√†nh 1 record dict (1 d√≤ng DataFrame).

    cover_url: l·∫•y t·ª´ imageLinks (thumbnail / smallThumbnail),
    KH√îNG t·ª± build t·ª´ id -> tr√°nh 404.
    """
    volume = item.get("volumeInfo", {}) or {}

    title = volume.get("title", "") or ""

    authors = volume.get("authors", []) or []
    author_name = ", ".join(authors)

    categories = volume.get("categories", []) or []
    categories_str = ", ".join(categories)

    description = volume.get("description", "") or ""
    published_date = volume.get("publishedDate", "") or ""
    language = volume.get("language", "") or ""
    publisher = volume.get("publisher", "") or ""

    page_count = volume.get("pageCount")
    average_rating = volume.get("averageRating")
    ratings_count = volume.get("ratingsCount")

    cover_url = extract_cover_url(volume)

    info_link = volume.get("infoLink", "") or item.get("selfLink", "") or ""

    isbn_10, isbn_13 = extract_isbns(volume)

    return {
        "title": title,
        "author_name": author_name,
        "categories": categories_str,
        "description": description,
        "published_date": published_date,
        "language": language,
        "publisher": publisher,
        "page_count": page_count,
        "average_rating": average_rating,
        "ratings_count": ratings_count,
        "isbn_10": isbn_10,
        "isbn_13": isbn_13,
        "cover_url": cover_url,
        "info_link": info_link,
    }


# ================== H√ÄM G·ªåI GOOGLE BOOKS API ==================

def search_google_books(
    query: str,
    *,
    lang: str = DEFAULT_LANG,
    max_results: int = 400,
    sleep_seconds: float = DEFAULT_SLEEP_SECONDS,
) -> List[Dict[str, Any]]:
    """
    T√¨m s√°ch v·ªõi Google Books API cho 1 query.
    - lang: m√£ ng√¥n ng·ªØ (vd: 'vi', 'en'...)
    - max_results: t·ªëi ƒëa s·ªë s√°ch mu·ªën l·∫•y cho ri√™ng query n√†y.

    CH·ªà tr·∫£ v·ªÅ c√°c row c√≥ cover_url (c√°c item kh√¥ng c√≥ imageLinks s·∫Ω b·ªã b·ªè qua).
    """
    api_key = get_api_key()
    all_rows: List[Dict[str, Any]] = []

    for start in range(0, max_results, MAX_PER_REQUEST):
        remaining = max_results - len(all_rows)
        if remaining <= 0:
            break

        per_request = min(MAX_PER_REQUEST, remaining)

        params = {
            "q": query,
            "langRestrict": lang,
            "printType": "books",
            "startIndex": start,
            "maxResults": per_request,
            "key": api_key,
        }

        print(f"[{query}] startIndex={start}, maxResults={per_request}")

        try:
            resp = requests.get(
                BASE_URL,
                params=params,
                timeout=20,
                headers={"User-Agent": "book-crawler/1.0"},
            )
        except Exception as e:
            print(f"  L·ªói k·∫øt n·ªëi: {e}")
            break

        if resp.status_code != 200:
            print("  L·ªói HTTP:", resp.status_code)
            try:
                print("  N·ªôi dung:", resp.text[:300])
            except Exception:
                pass
            break

        data = resp.json()
        total_items = data.get("totalItems", 0)
        items = data.get("items") or []

        print("  totalItems (cho query n√†y):", total_items)
        print("  S·ªë items nh·∫≠n ƒë∆∞·ª£c:", len(items))

        if not items:
            break

        for item in items:
            row = extract_book_from_item(item)

            # CH·ªà gi·ªØ s√°ch c√≥ cover_url kh√¥ng r·ªóng
            cover_url = (row.get("cover_url") or "").strip()
            if not cover_url:
                continue

            all_rows.append(row)
            if len(all_rows) >= max_results:
                return all_rows

        time.sleep(sleep_seconds)

    return all_rows


def deduplicate_books(df: pd.DataFrame) -> pd.DataFrame:
    """
    Lo·∫°i b·ªè tr√πng:
    1. V·ªõi s√°ch c√≥ ISBN_13 -> drop_duplicates theo isbn_13
    2. V·ªõi s√°ch kh√¥ng c√≥ ISBN_13 -> drop_duplicates theo (title, author_name, published_date)
    """
    for col in ["isbn_13", "title", "author_name", "published_date"]:
        if col not in df.columns:
            df[col] = ""

    df["isbn_13"] = df["isbn_13"].astype(str).str.strip()
    df["title"] = df["title"].astype(str).str.strip()
    df["author_name"] = df["author_name"].astype(str).str.strip()
    df["published_date"] = df["published_date"].astype(str).str.strip()

    df_has_isbn = df[df["isbn_13"] != ""].copy()
    df_no_isbn = df[df["isbn_13"] == ""].copy()

    before_has = len(df_has_isbn)
    df_has_isbn.drop_duplicates(subset=["isbn_13"], inplace=True)
    after_has = len(df_has_isbn)
    print(f"Lo·∫°i tr√πng theo ISBN_13: {before_has} -> {after_has}")

    before_no = len(df_no_isbn)
    df_no_isbn.drop_duplicates(
        subset=["title", "author_name", "published_date"], inplace=True
    )
    after_no = len(df_no_isbn)
    print(
        f"Lo·∫°i tr√πng theo (title, author_name, published_date): "
        f"{before_no} -> {after_no}"
    )

    df_final = pd.concat([df_has_isbn, df_no_isbn], ignore_index=True)
    print("T·ªïng s·ªë s√°ch sau khi g·ªôp & lo·∫°i tr√πng:", len(df_final))
    return df_final


# ================== H√ÄM TEST 1 S√ÅCH ==================

def test_one_book(query: str = "s√°ch", lang: str = DEFAULT_LANG):
    """
    T√¨m 1 s√°ch (c√≥ cover_url) ƒë·ªÉ test,
    in ra title, cover_url v√† HTTP status c·ªßa cover_url.
    """
    print("=== TEST 1 S√ÅCH ===")
    rows = search_google_books(query, lang=lang, max_results=1)
    if not rows:
        print("Kh√¥ng t√¨m ƒë∆∞·ª£c s√°ch n√†o c√≥ cover_url cho query:", query)
        return

    book = rows[0]
    title = book.get("title")
    cover_url = book.get("cover_url")

    print("Ti√™u ƒë·ªÅ:", title)
    print("Cover URL:", cover_url)

    try:
        r = requests.get(cover_url, timeout=20)
        print("HTTP status cover_url:", r.status_code)
    except Exception as e:
        print("L·ªói khi request cover_url:", e)


# ================== MAIN CRAWL NHI·ªÄU QUERY ==================

def main():
    queries = [
        # VƒÉn h·ªçc
        "ti·ªÉu thuy·∫øt",
        "truy·ªán ng·∫Øn",
        "ng√¥n t√¨nh",
        "trinh th√°m",
        "vƒÉn h·ªçc Vi·ªát Nam",
        "vƒÉn h·ªçc n∆∞·ªõc ngo√†i",
        # Kinh t·∫ø ‚Äì t√†i ch√≠nh ‚Äì kinh doanh
        "kinh t·∫ø",
        "marketing",
        "t√†i ch√≠nh",
        "ƒë·∫ßu t∆∞",
        "qu·∫£n tr·ªã kinh doanh",
        "kh·ªüi nghi·ªáp",
        # K·ªπ nƒÉng ‚Äì t√¢m l√Ω
        "k·ªπ nƒÉng s·ªëng",
        "t√¢m l√Ω h·ªçc",
        "ph√°t tri·ªÉn b·∫£n th√¢n",
        # L·ªãch s·ª≠ ‚Äì vƒÉn h√≥a ‚Äì t√¥n gi√°o
        "l·ªãch s·ª≠",
        "vƒÉn h√≥a",
        "t√¥n gi√°o",
        # Khoa h·ªçc ‚Äì c√¥ng ngh·ªá
        "khoa h·ªçc",
        "khoa h·ªçc t·ª± nhi√™n",
        "khoa h·ªçc x√£ h·ªôi",
        "c√¥ng ngh·ªá th√¥ng tin",
        "l·∫≠p tr√¨nh",
        # Thi·∫øu nhi ‚Äì gi√°o d·ª•c
        "thi·∫øu nhi",
        "truy·ªán tranh",
        "s√°ch gi√°o khoa",
        "h·ªçc ngo·∫°i ng·ªØ",
        "ti·∫øng Anh",
    ]

    all_rows: List[Dict[str, Any]] = []
    MAX_PER_QUERY = 400

    for q in queries:
        print("=" * 60)
        print(f"ƒêang ch·∫°y query: '{q}'")
        try:
            rows = search_google_books(
                q, lang=DEFAULT_LANG, max_results=MAX_PER_QUERY
            )
        except Exception as e:
            print(f"  L·ªói khi x·ª≠ l√Ω query '{q}': {e}")
            continue

        print(
            f"==> Query '{q}' l·∫•y ƒë∆∞·ª£c {len(rows)} s√°ch (ƒë√£ l·ªçc ch·ªâ s√°ch c√≥ cover)\n"
        )
        all_rows.extend(rows)

    if not all_rows:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c l·∫•y. Tho√°t.")
        return

    df = pd.DataFrame(all_rows)

    # L·ªåC L·∫†I M·ªòT L·∫¶N N·ªÆA (CHO CH·∫ÆC): ch·ªâ gi·ªØ s√°ch c√≥ cover_url
    df["cover_url"] = df["cover_url"].astype(str).str.strip()
    df = df[df["cover_url"] != ""].copy()
    print("Sau khi ƒë·∫£m b·∫£o ch·ªâ gi·ªØ s√°ch c√≥ cover_url:", len(df))

    df_final = deduplicate_books(df)

    # N·∫øu v·∫´n c√≤n &amp; ƒë√¢u ƒë√≥ (hi·∫øm), th√¨ g·ª° lu√¥n
    df_final["cover_url"] = (
        df_final["cover_url"].astype(str).str.replace(
            "&amp;", "&", regex=False)
    )

    output_file = "googlebooks_vi_multi_enriched.xlsx"
    df_final.to_excel(output_file, index=False)
    print(f"ƒê√£ l∆∞u file: {output_file}")


if __name__ == "__main__":
    # üëâ ƒê·ªÇ TEST 1 S√ÅCH:
    main()

    # üëâ Khi crawl th·∫≠t th√¨ comment d√≤ng tr√™n v√† m·ªü d√≤ng d∆∞·ªõi:
    # main()
